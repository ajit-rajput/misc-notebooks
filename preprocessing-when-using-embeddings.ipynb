{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I want to illustrate how I do come up with meaningful preprocessing when building deep learning NLP models.\n\nI start with two golden rules:\n\n1) Don't use standard preprocessing steps like stemming or stopword removal when you have pre-trained embeddings <br>\n2) Get your vocabulary as close to the embeddings as possible\n\nSome of you might used standard preprocessing steps when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc. The reason is simple: You loose valuable information, which would help your NN to figure things out.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:27:42.899098Z","iopub.execute_input":"2021-08-05T17:27:42.899541Z","iopub.status.idle":"2021-08-05T17:27:42.906259Z","shell.execute_reply.started":"2021-08-05T17:27:42.899503Z","shell.execute_reply":"2021-08-05T17:27:42.904967Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/bug-data/bugs-data.csv\")\nprint(\"Train shape : \",train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:25:48.283671Z","iopub.execute_input":"2021-08-05T17:25:48.284035Z","iopub.status.idle":"2021-08-05T17:25:48.300810Z","shell.execute_reply.started":"2021-08-05T17:25:48.284006Z","shell.execute_reply":"2021-08-05T17:25:48.299731Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train shape :  (2764, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:26:27.960058Z","iopub.execute_input":"2021-08-05T17:26:27.960453Z","iopub.status.idle":"2021-08-05T17:26:27.988652Z","shell.execute_reply.started":"2021-08-05T17:26:27.960422Z","shell.execute_reply":"2021-08-05T17:26:27.987963Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     bugid                                            summary  \\\n0   808252  home and end key not working on web pages open...   \n1  1389748        [Linux] default browser firefox not working   \n2  1588476  Full page scrolling screenshot not working for...   \n3  1609740  Redhat Linux EL 6.10 Firefox v68, Set as Deskt...   \n4  1617503  Bookmark button not working after system retur...   \n\n             component  \n0  Keyboard Navigation  \n1    Shell Integration  \n2           PDF Viewer  \n3    Shell Integration  \n4  Bookmarks & History  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bugid</th>\n      <th>summary</th>\n      <th>component</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>808252</td>\n      <td>home and end key not working on web pages open...</td>\n      <td>Keyboard Navigation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1389748</td>\n      <td>[Linux] default browser firefox not working</td>\n      <td>Shell Integration</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1588476</td>\n      <td>Full page scrolling screenshot not working for...</td>\n      <td>PDF Viewer</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1609740</td>\n      <td>Redhat Linux EL 6.10 Firefox v68, Set as Deskt...</td>\n      <td>Shell Integration</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1617503</td>\n      <td>Bookmark button not working after system retur...</td>\n      <td>Bookmarks &amp; History</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"I will use the following function to track our training vocabulary, which goes through all our text and counts the occurance of the contained words.","metadata":{}},{"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:27:14.181545Z","iopub.execute_input":"2021-08-05T17:27:14.181892Z","iopub.status.idle":"2021-08-05T17:27:14.188223Z","shell.execute_reply.started":"2021-08-05T17:27:14.181865Z","shell.execute_reply":"2021-08-05T17:27:14.186949Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"So lets populate the vocabulary and display the first 5 elements and their count. Note that now we can use progess_apply to see progress bar","metadata":{}},{"cell_type":"code","source":"sentences = train[\"summary\"].progress_apply(lambda x: x.split()).values\nvocab = build_vocab(sentences)\nprint({k: vocab[k] for k in list(vocab)[:5]})","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:27:46.629424Z","iopub.execute_input":"2021-08-05T17:27:46.629813Z","iopub.status.idle":"2021-08-05T17:27:46.673411Z","shell.execute_reply.started":"2021-08-05T17:27:46.629781Z","shell.execute_reply":"2021-08-05T17:27:46.672118Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 2764/2764 [00:00<00:00, 247656.67it/s]\n100%|██████████| 2764/2764 [00:00<00:00, 269005.39it/s]","output_type":"stream"},{"name":"stdout","text":"{'home': 15, 'and': 349, 'end': 5, 'key': 20, 'not': 1948}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Next we import the embeddings we want to use in our model later. For illustration I use GoogleNews here.","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nnews_path = '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:34:46.462003Z","iopub.execute_input":"2021-08-05T17:34:46.462472Z","iopub.status.idle":"2021-08-05T17:36:08.901131Z","shell.execute_reply.started":"2021-08-05T17:34:46.462412Z","shell.execute_reply":"2021-08-05T17:36:08.899588Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Next I define a function that checks the intersection between our vocabulary and the embeddings. It will output a list of out of vocabulary (oov) words that we can use to improve our preprocessing","metadata":{}},{"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:36:18.581094Z","iopub.execute_input":"2021-08-05T17:36:18.581598Z","iopub.status.idle":"2021-08-05T17:36:18.728457Z","shell.execute_reply.started":"2021-08-05T17:36:18.581550Z","shell.execute_reply":"2021-08-05T17:36:18.726905Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"oov = check_coverage(vocab,embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:36:28.548236Z","iopub.execute_input":"2021-08-05T17:36:28.548637Z","iopub.status.idle":"2021-08-05T17:36:28.592895Z","shell.execute_reply.started":"2021-08-05T17:36:28.548604Z","shell.execute_reply":"2021-08-05T17:36:28.591869Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"100%|██████████| 5581/5581 [00:00<00:00, 168178.37it/s]","output_type":"stream"},{"name":"stdout","text":"Found embeddings for 53.70% of vocab\nFound embeddings for  81.97% of all text\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Only 53% of our vocabulary will have embeddings, making our data more or less useless. So lets have a look and start improving. For this we can easily have a look at the top oov words.","metadata":{}},{"cell_type":"code","source":"oov[:20]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:36:54.638991Z","iopub.execute_input":"2021-08-05T17:36:54.639399Z","iopub.status.idle":"2021-08-05T17:36:54.647094Z","shell.execute_reply.started":"2021-08-05T17:36:54.639352Z","shell.execute_reply":"2021-08-05T17:36:54.645890Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[('and', 349),\n ('to', 291),\n ('a', 167),\n ('available/working', 133),\n ('of', 119),\n ('working.', 91),\n ('-', 72),\n ('working,', 43),\n ('3.5', 30),\n ('[e10s]', 21),\n ('add-on', 19),\n ('e10s', 19),\n ('/', 18),\n ('10', 17),\n ('properly.', 12),\n ('\"Open', 12),\n ('3.6', 10),\n ('working)', 10),\n (',', 9),\n ('working:', 8)]"},"metadata":{}}]},{"cell_type":"markdown","source":"On first place there is \"to\". Why? Simply because \"to\" was removed when the GoogleNews Embeddings were trained. We will fix this later, for now we take care about the splitting of punctuation as this also seems to be a Problem. But what do we do with the punctuation then - Do we want to delete or consider as a token? I would say: It depends. If the token has an embedding, keep it, if it doesn't we don't need it anymore. So lets check:","metadata":{}},{"cell_type":"code","source":"'?' in embeddings_index","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:48:37.992304Z","iopub.execute_input":"2021-08-05T17:48:37.992677Z","iopub.status.idle":"2021-08-05T17:48:37.998920Z","shell.execute_reply.started":"2021-08-05T17:48:37.992647Z","shell.execute_reply":"2021-08-05T17:48:37.997871Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"'&' in embeddings_index","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:48:44.202693Z","iopub.execute_input":"2021-08-05T17:48:44.203040Z","iopub.status.idle":"2021-08-05T17:48:44.209858Z","shell.execute_reply.started":"2021-08-05T17:48:44.203012Z","shell.execute_reply":"2021-08-05T17:48:44.208504Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"Interesting. While \"&\" is in the Google News Embeddings, \"?\" is not. So we basically define a function that splits off \"&\" and removes other punctuation.","metadata":{}},{"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:37:21.472804Z","iopub.execute_input":"2021-08-05T17:37:21.473181Z","iopub.status.idle":"2021-08-05T17:37:21.480050Z","shell.execute_reply.started":"2021-08-05T17:37:21.473150Z","shell.execute_reply":"2021-08-05T17:37:21.478741Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train[\"clean_summary\"] = train[\"summary\"].progress_apply(lambda x: clean_text(x))\nsentences = train[\"clean_summary\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:37:58.080232Z","iopub.execute_input":"2021-08-05T17:37:58.080633Z","iopub.status.idle":"2021-08-05T17:37:58.142366Z","shell.execute_reply.started":"2021-08-05T17:37:58.080602Z","shell.execute_reply":"2021-08-05T17:37:58.141212Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"100%|██████████| 2764/2764 [00:00<00:00, 86782.82it/s]\n100%|██████████| 2764/2764 [00:00<00:00, 286956.84it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"oov = check_coverage(vocab,embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:38:06.979079Z","iopub.execute_input":"2021-08-05T17:38:06.979525Z","iopub.status.idle":"2021-08-05T17:38:07.014848Z","shell.execute_reply.started":"2021-08-05T17:38:06.979488Z","shell.execute_reply":"2021-08-05T17:38:07.013607Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"100%|██████████| 4390/4390 [00:00<00:00, 209145.88it/s]","output_type":"stream"},{"name":"stdout","text":"Found embeddings for 74.05% of vocab\nFound embeddings for  90.30% of all text\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Nice! We were able to increase our embeddings ratio from 53% to 74% by just handling punctiation. Ok lets check on thos oov words.","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    print(embeddings_index.index_to_key[i])","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:38:53.139230Z","iopub.execute_input":"2021-08-05T17:38:53.139835Z","iopub.status.idle":"2021-08-05T17:38:53.147482Z","shell.execute_reply.started":"2021-08-05T17:38:53.139797Z","shell.execute_reply":"2021-08-05T17:38:53.145598Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"</s>\nin\nfor\nthat\nis\non\n##\nThe\nwith\nsaid\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Hmm seems like numbers also are a problem. Lets check the top 10 embeddings to get a clue.\nThere is \"##\" in there - Simply because as a reprocessing all numbers bigger than 9 have been replaced by hashs. I.e. 15 becomes ## while 123 becomes ### or 15.80€ becomes ##.##€. So lets mimic this preprocessing step to further improve our embeddings coverage","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:39:36.469650Z","iopub.execute_input":"2021-08-05T17:39:36.470331Z","iopub.status.idle":"2021-08-05T17:39:36.475695Z","shell.execute_reply.started":"2021-08-05T17:39:36.470278Z","shell.execute_reply":"2021-08-05T17:39:36.474878Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train[\"clean_summary\"] = train[\"clean_summary\"].progress_apply(lambda x: clean_numbers(x))\nsentences = train[\"clean_summary\"].progress_apply(lambda x: x.split())\nvocab = build_vocab(sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:40:02.378988Z","iopub.execute_input":"2021-08-05T17:40:02.379690Z","iopub.status.idle":"2021-08-05T17:40:02.529151Z","shell.execute_reply.started":"2021-08-05T17:40:02.379631Z","shell.execute_reply":"2021-08-05T17:40:02.528464Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 2764/2764 [00:00<00:00, 58342.76it/s]\n100%|██████████| 2764/2764 [00:00<00:00, 35901.49it/s]\n100%|██████████| 2764/2764 [00:00<00:00, 289421.22it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"oov = check_coverage(vocab,embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:40:11.748611Z","iopub.execute_input":"2021-08-05T17:40:11.749290Z","iopub.status.idle":"2021-08-05T17:40:11.781606Z","shell.execute_reply.started":"2021-08-05T17:40:11.749234Z","shell.execute_reply":"2021-08-05T17:40:11.780258Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 4094/4094 [00:00<00:00, 194401.46it/s]","output_type":"stream"},{"name":"stdout","text":"Found embeddings for 80.09% of vocab\nFound embeddings for  92.65% of all text\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Nice! Another 6% increase. Now as much as with handling the puntuation, but every bit helps. Lets check the oov words","metadata":{}},{"cell_type":"code","source":"oov[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:42:58.351296Z","iopub.execute_input":"2021-08-05T17:42:58.351788Z","iopub.status.idle":"2021-08-05T17:42:58.360069Z","shell.execute_reply.started":"2021-08-05T17:42:58.351744Z","shell.execute_reply":"2021-08-05T17:42:58.358700Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[('and', 353),\n ('to', 301),\n ('a', 169),\n ('of', 119),\n ('e##s', 40),\n ('E##S', 11),\n ('###a1', 9),\n ('firefox2', 6),\n ('userChromecss', 5),\n ('windowopen', 5)]"},"metadata":{}}]},{"cell_type":"code","source":"to_remove = ['a','to','of','and']\nsentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\nvocab = build_vocab(sentences)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:43:09.242095Z","iopub.execute_input":"2021-08-05T17:43:09.242496Z","iopub.status.idle":"2021-08-05T17:43:09.274592Z","shell.execute_reply.started":"2021-08-05T17:43:09.242465Z","shell.execute_reply":"2021-08-05T17:43:09.273814Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"100%|██████████| 2764/2764 [00:00<00:00, 234843.64it/s]\n100%|██████████| 2764/2764 [00:00<00:00, 312001.94it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"oov = check_coverage(vocab,embeddings_index)","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:43:18.272025Z","iopub.execute_input":"2021-08-05T17:43:18.272413Z","iopub.status.idle":"2021-08-05T17:43:18.309940Z","shell.execute_reply.started":"2021-08-05T17:43:18.272377Z","shell.execute_reply":"2021-08-05T17:43:18.308768Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 4090/4090 [00:00<00:00, 149079.29it/s]","output_type":"stream"},{"name":"stdout","text":"Found embeddings for 80.17% of vocab\nFound embeddings for  96.11% of all text\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"oov[:20]","metadata":{"execution":{"iopub.status.busy":"2021-08-05T17:43:33.681136Z","iopub.execute_input":"2021-08-05T17:43:33.681502Z","iopub.status.idle":"2021-08-05T17:43:33.688356Z","shell.execute_reply.started":"2021-08-05T17:43:33.681471Z","shell.execute_reply":"2021-08-05T17:43:33.687548Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[('e##s', 40),\n ('E##S', 11),\n ('###a1', 9),\n ('firefox2', 6),\n ('userChromecss', 5),\n ('windowopen', 5),\n ('Firefox##', 4),\n ('###a2', 4),\n ('windowfocus', 4),\n ('##b2', 4),\n ('PgUp', 4),\n ('Cannot', 4),\n ('setTimeout', 4),\n ('Quantumbar', 3),\n ('pdfjs', 3),\n ('PageUp', 3),\n ('Forecastfox', 3),\n ('ctrlT', 3),\n ('Autoscroll', 3),\n ('DownThemAll', 3)]"},"metadata":{}}]},{"cell_type":"markdown","source":"This Looks good now for getting as closer to embeddings.","metadata":{}}]}