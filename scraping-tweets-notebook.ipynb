{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### How to Scrape Tweets from Twitter with Python Twint","metadata":{}},{"cell_type":"markdown","source":"### Scraping Tweets using TWINT","metadata":{}},{"cell_type":"markdown","source":"Twint is an advanced Twitter scraping tool written in Python that allows for scraping Tweets from Twitter.\n\nThe advantage of Twint is that you don’t need Twitter’s API to make TWINT work. \n\nReference Configuration: https://github.com/twintproject/twint/wiki/Configuration","metadata":{}},{"cell_type":"code","source":"!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n\n!pip install nest_asyncio","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:39:03.931963Z","iopub.execute_input":"2021-08-13T05:39:03.932386Z","iopub.status.idle":"2021-08-13T05:39:30.032975Z","shell.execute_reply.started":"2021-08-13T05:39:03.932348Z","shell.execute_reply":"2021-08-13T05:39:30.031696Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting twint\n  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /tmp/pip-install-f_ln91_3/twint_365ac05fcb3b4a3a80babf6c60e4ea17\n  Running command git clone -q https://github.com/twintproject/twint.git /tmp/pip-install-f_ln91_3/twint_365ac05fcb3b4a3a80babf6c60e4ea17\n\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n  Running command git checkout -q origin/master\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from twint) (3.7.4.post0)\nCollecting aiodns\n  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from twint) (4.9.3)\nCollecting cchardet\n  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n\u001b[K     |████████████████████████████████| 263 kB 519 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from twint) (0.6)\nCollecting elasticsearch\n  Downloading elasticsearch-7.14.0-py2.py3-none-any.whl (364 kB)\n\u001b[K     |████████████████████████████████| 364 kB 6.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pysocks in /opt/conda/lib/python3.7/site-packages (from twint) (1.7.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from twint) (1.2.4)\nCollecting aiohttp_socks\n  Downloading aiohttp_socks-0.6.0-py3-none-any.whl (9.2 kB)\nCollecting schedule\n  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: geopy in /opt/conda/lib/python3.7/site-packages (from twint) (2.1.0)\nCollecting fake-useragent\n  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\nCollecting googletransx\n  Downloading googletransx-2.4.2.tar.gz (13 kB)\nCollecting pycares>=4.0.0\n  Downloading pycares-4.0.0-cp37-cp37m-manylinux2010_x86_64.whl (291 kB)\n\u001b[K     |████████████████████████████████| 291 kB 15.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from pycares>=4.0.0->aiodns->twint) (1.14.5)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.20)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (5.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (1.6.3)\nRequirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (4.0.0)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (3.7.4.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->twint) (21.2.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\nCollecting python-socks[asyncio]>=1.2.2\n  Downloading python_socks-1.2.4-py3-none-any.whl (35 kB)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->twint) (2.2.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from elasticsearch->twint) (2021.5.30)\nRequirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from elasticsearch->twint) (1.26.5)\nRequirement already satisfied: geographiclib<2,>=1.49 in /opt/conda/lib/python3.7/site-packages (from geopy->twint) (1.52)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from googletransx->twint) (2.25.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->twint) (2.8.1)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->twint) (2021.1)\nRequirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas->twint) (1.19.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\nBuilding wheels for collected packages: twint, fake-useragent, googletransx\n  Building wheel for twint (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38861 sha256=f774ae3c233db7955467210285e6a16e1ddaf51e434ddc7234b3b56f0552592d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-stj4d_7k/wheels/8d/dc/9f/74b4483d5f997036f04aec7f42bd4b3c80f04264920c368068\n  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13486 sha256=0d4fd9607d84bc4eb0490c2a1431a3df6f4a82d0e5e2062f8834983984bddf9a\n  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n  Building wheel for googletransx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15969 sha256=6ca04f7de734c47ab9d15a9844917b376f48ad6150bdc27e3d8efa34e44ad082\n  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\nSuccessfully built twint fake-useragent googletransx\nInstalling collected packages: python-socks, pycares, schedule, googletransx, fake-useragent, elasticsearch, cchardet, aiohttp-socks, aiodns, twint\n\u001b[33m  WARNING: The script twint is installed in '/root/.local/bin' which is not on PATH.\n  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\nSuccessfully installed aiodns-3.0.0 aiohttp-socks-0.6.0 cchardet-2.1.7 elasticsearch-7.14.0 fake-useragent-0.1.11 googletransx-2.4.2 pycares-4.0.0 python-socks-1.2.4 schedule-1.1.0 twint-2.1.21\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.7/site-packages (1.5.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import twint\nimport nest_asyncio\nnest_asyncio.apply()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:39:30.035339Z","iopub.execute_input":"2021-08-13T05:39:30.035670Z","iopub.status.idle":"2021-08-13T05:39:30.581756Z","shell.execute_reply.started":"2021-08-13T05:39:30.035634Z","shell.execute_reply":"2021-08-13T05:39:30.580719Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"In this article, I’ll describe how to loop and scrape tweets from list of cities","metadata":{}},{"cell_type":"code","source":"all_cities = [\"Mumbai\", \"Pune\", \"Bengaluru\", \"Delhi\", \"Lucknow\", \"Hyderabad\", \"Jaipur\", \"Goa\"]\n\ndef scrape_by_city(keywords, since, until, outfile):\n    unique_cities=set(all_cities) #To get unique cities of country\n    cities = sorted(unique_cities) #Sort & convert datatype to list\n    for city in cities:\n        print(city)\n        c = twint.Config()\n        c.Search = keywords #search keyword\n        c.Since = since\n        c.Until = until\n        c.Store_csv = True\n        c.Lang = \"en\"\n        c.Limit = 200\n        c.Output = \"./\" + outfile\n        c.Near = city\n        c.Hide_output = True\n        c.Count = True\n        c.Stats = True\n        c.Resume = 'resume3.txt'\n        twint.run.Search(c)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:42:46.563968Z","iopub.execute_input":"2021-08-13T05:42:46.564348Z","iopub.status.idle":"2021-08-13T05:42:46.571447Z","shell.execute_reply.started":"2021-08-13T05:42:46.564317Z","shell.execute_reply":"2021-08-13T05:42:46.570511Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"scrape_by_city('', '2021-01-01 00:00:00','2021-01-01 23:00:00','IN_City_Tweets_Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T05:42:50.967252Z","iopub.execute_input":"2021-08-13T05:42:50.967600Z","iopub.status.idle":"2021-08-13T05:43:42.821896Z","shell.execute_reply.started":"2021-08-13T05:42:50.967571Z","shell.execute_reply":"2021-08-13T05:43:42.820686Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Bengaluru\n[+] Finished: Successfully collected 213 Tweets.\nDelhi\n[+] Finished: Successfully collected 219 Tweets.\nGoa\n[+] Finished: Successfully collected 219 Tweets.\nHyderabad\n[+] Finished: Successfully collected 200 Tweets.\nJaipur\n[+] Finished: Successfully collected 200 Tweets.\nLucknow\n[+] Finished: Successfully collected 200 Tweets.\nMumbai\n[+] Finished: Successfully collected 219 Tweets.\nPune\n[!] No more data! Scraping will stop now.\nfound 0 deleted tweets in this search.\n[+] Finished: Successfully collected 137 Tweets.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, Let me explain the ‘scrape_by_city()’ method.\n\nFirst of all, we have to import twint configuration.\n\n#Keyword: You can keep the keyword empty if required, as in this way it will scrape all the tweets. Otherwise, If you are targeting a specific keyword you can add it there.\n\n#Since: At this argument, you add the timestamp from when you want to scrape the tweets.\n\n#Store_csv: Enable to save the tweets in the CSV file.\n\n#Output: Enter the filename under you want to sav the dataset.\n\n#Near: Here you have to enter the city name.\n\n#Hide_output: If you don’t want to see the output in the terminal which I prefer in production because it slows down the performance.\n\n#Count: To check the status of the number of tweets scraped.\n\n#Resume: To resume the scraping we have to pass the last id value stored in our CSV or the place where we are storing the \ndata. The script below will help you get the last id.\n\n<code>\n#script to get last id value\nimport pandas as pd\nimport numpy as np\ndf = pd.reas_csv('last_hour.csv')\nresult = df.iloc[[-1]]\nprint(result) </code>\n<br><br>\n\nAfter getting the “id” value which will be a number like this “1298337878339133447\". You have to save the number into a file like ‘resume.txt’. Place the resume file into the same directory of the original scraping script and assign the filename to the argument. Now run the script and guess what scraping has been restarted.","metadata":{}}]}